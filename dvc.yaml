# List experiment stages, i.e., the jobs to be run
stages:
  # Stage running model training
  train:
    # Stage runs the training script as the command
    #   Note: On Noctua 2 this requires a GPU node for training
    cmd: PARTITION=gpu bash run.sh python -m train
    # Data dependencies of this stage to determine when it needs to be rerun
    deps:
      # Run scripts orchestrating the script execution (potentially via sbatch)
      - run.sh
      - noctua.sh
      # The training and evaluation dataset
      - ${dataset.path}
      # The model, dataset and training python scripts
      - model.py
      - activations.py
      - dataset.py
      - train.py
    # Parameters used by this stage, changing any of these triggers reruns
    params:
      # Track the model, dataset and training hyperparameter sections from the
      # parameters file
      - model
      - dataset
      - train
      - seed
    # Outputs produced which should be tracked and passed on to the next stages
    outs:
      # The trained model checkpoint
      - outputs/model.pt
      # The optimizer state after training
      - outputs/optimizer.pt
    # Plots produced by this stage
    plots:
      # Curves of training and validation loss per epoch
      - loss.yaml:
          # Explicitly plot the validation loss
          y: valid
          # Give a more readable title to the plot
          title: "Validation Loss per Epoch"
          # Track via git, not dvc cache
          cache: false
  # Stage running model evaluation after training to produce accuracy metrics
  eval:
    # Stage runs the evaluation script as the command
    cmd: python -m eval
    # Data dependencies of this stage to determine when it needs to be rerun
    deps:
      # The training and evaluation dataset
      - ${dataset.path}
      # The model checkpoint produced by the training stage
      - outputs/model.pt
      # The model, dataset and evaluation python scripts
      - model.py
      - activations.py
      - dataset.py
      - eval.py
    # Parameters used by this stage, changing any of these triggers reruns
    params:
      # Track the model, dataset and eval hyperparameter sections from the
      # parameters file
      - model
      - dataset
      - eval
      - seed
    # Metrics produced by this stage
    metrics:
      # Classification accuracy over the evaluation dataset
      - accuracy.yaml:
          # Track via git, not dvc cache
          cache: false
    # Plots produced by this stage
    plots:
      # Accuracy grouped by Signal-to-Noise Ratio
      - accuracy-per-snr.yaml:
          # Use the Signal-to_noise Ration levels as x-axis
          x: snr
          # Plot accuracy per SNR level as y-axis
          y: acc
          # Give a more readable title to the plot
          title: "Accuracy per SNR"
          # Track via git, not dvc cache
          cache: false
      # Confusion matrix of predicted vs. true classes
      - classes.csv:
          # Use true class label as x-axis
          x: cls
          # Use the predicted class label as y-axis
          y: prediction
          # Use the confusion matrix plot template
          template: confusion
          # Give a more readable title to the plot
          title: "Confusion Matrix"
          # Do not track via git as this file might be quite large, up to some
          # megabytes
          cache: true
  # Stage exporting the trained model to ONNX
  export:
    # Stage runs the export script as the command
    cmd: python -m export
    # Data dependencies of this stage to determine when it needs to be rerun
    deps:
      # The training and evaluation dataset
      - ${dataset.path}
      # The model checkpoint produced by the training stage
      - outputs/model.pt
      # The model, dataset and evaluation python scripts
      - model.py
      - activations.py
      - dataset.py
      - export.py
    # Parameters used by this stage, changing any of these triggers reruns
    params:
      # Track the model, dataset and export hyperparameter sections from the
      # parameters file
      - model
      - dataset
      - export
      - seed
    # Outputs produced which should be tracked and passed on to the next stages
    outs:
      # The exported model onnx graph file
      - outputs/model.onnx
      # Sample input-output pair for verification
      - outputs/inp.npy
      - outputs/out.npy
  # Stage running the finn dataflow accelerator build of the model
  build:
    # Command running a finn build of the model produced during export
    #   Note: More options should be provided via environment variables
    cmd: bash run.sh 'finn run -d .finn build.py'
    # Dependencies of this stage to determine when it needs to be rerun
    deps:
      # Run scripts orchestrating the script execution (potentially via sbatch)
      - run.sh
      - noctua.sh
      # The script executing the stage
      - build.py
      # The custom build steps implemented in this package
      - build_steps.py
      # Implementation details of the new custom build steps
      - custom/apply_config.py
      # The model export produced by the export stage: This triggers rerunning
      # this stage when the model configuration changes
      - outputs/model.onnx
      # Input/Output pair in numpy format for verification during the build
      - outputs/inp.npy
      - outputs/out.npy
    # Parameters used by this stage
    params:
      # Random number generator seed from params.yaml for reproducibility
      - seed
      # Track the build section from the parameters file
      - build
      # Folding configurations specifying parallelization and FIFO sizes for the
      # layers/operators
      - ${build.finn.folding_config_file}:
      # Configuration file specifying the preferred implementation style of
      # custom operators
      - ${build.finn.specialize_layers_config_file}:
    # Outputs produced by this stage which should be tracked and passed on to
    # the next stage
    outs:
      # Track everything from the build directory
      - ${build.finn.output_dir}/
    # Metrics produced by this stage
    metrics:
      # Resource utilization metrics
      - resources.yaml:
          # Track via git, not dvc cache
          cache: false
  # Stage checking the verification output of the FINN dataflow build
  verify:
    # Command summarizing verification outputs from the build output products
    cmd: python -m verify
    # Dependencies of this stage to determine when it needs to be rerun
    deps:
      # The script executing the stage
      - verify.py
      # The output products of the previous stage
      - ${build.finn.output_dir}/verification_output/
    # Parameters used by this stage
    params:
      # Track the build section from the parameters file
      - build.finn.output_dir
    # Produces metrics as output
    metrics:
      # Track all metrics in this file
      - verification.yaml:
          # Keep the output tracked by git
          cache: false
