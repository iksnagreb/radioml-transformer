# Tagging of metrics and verification aggregation to assist filtering experiment
# sweeps
tag: layers
# Global seed to make training and evaluation reproducible
# Note: Dataset splitting has its own seed
seed: 67
# Model configuration section
model:
  # Number of attention heads
  num_heads: 8
  # Number of attention block "layers"
  num_layers: 1
  # Enables/disables bias on linear layers
  bias: true
  # Size of the input/output embedding dimension
  emb_dim: 64
  # Size of the MLP layer dimension
  mlp_dim: 256
  # Type of normalization layer to use in the transformer blocks
  #   Options are: layer-norm, batch-norm and none
  norm: batch-norm
  # Type of positional encoding to use at the input
  #   Options are: none, sinusoidal, binary, learned
  positional_encoding: binary
  # Dropout: probability of an element to be zeroed during training
  dropout: 0.0
  # Number of bits to use for quantized representation in "almost all layers"
  bits: 4
  # Quantization bit-width at the model inputs: Typically this should
  # be higher than for most other layers, e.g., keep this at 8 bits
  input_bits: 8
  # Quantization bit-width at the model outputs: Typically this should
  # be higher than for most other layers, e.g., keep this at 8 bits
  output_bits: 8
  # Number of classes at the classification head
  num_classes: 24
# Training/Validation dataset configuration
dataset:
  # Path to the dataset file
  path: data/GOLD_XYZ_OSC.0001_1024.hdf5
  # Optionally select only a subset of the classes
  #  classes: null
  # Optionally select only a subset of the available noise levels
  signal_to_noise_ratios:
  - -6
  - -4
  - -2
  - 0
  - 2
  - 4
  - 6
  - 8
  - 10
  - 12
  - 14
  - 16
  - 18
  - 20
  - 22
  - 24
  - 26
  - 28
  - 30
  # Splits the dataset into train/validation/evaluation subsets
  splits:                       # 80%, 10%, 10% splits
  # Seed used for reproducible splitting of datasets
  - 0.80
  - 0.10
  - 0.10
  seed: 12
  # Reshape the dataset to fit the expected embedding dimension of the model
  # and to effectively reduce the sequence length
  reshape:             # -1 will infer the sequence length
# Training hyperparameters
  - -1
  - 64
train:
  # Batch size for training
  batch_size: 512
  # Number of training epochs to run
  epochs: 100
  # Optimizer configuration
  optimizer:
    # Name of the optimization algorithm to use
    algorithm: adam
    # (Initial) Learning rate
    lr: 0.001
    # L2 regularization
    weight_decay: 0.0
    # Coefficients used for computing running averages of gradient and its
    # square.
    # Note: Set according to Vaswani et al. 2017
    betas:
    - 0.9
    - 0.98
    # Term added to the denominator to improve numerical stability
    # Note: Set according to Vaswani et al. 2017
    eps: 1.0e-9
  # Loss function to use
  criterion: cross-entropy
  # DataLoader keyword arguments
  loader:
    # Reshuffle data every epoch
    shuffle: true
    # Number of workers to use for loading the data in parallel
    num_workers: 10
    # Number of batches loaded in advance by each worker
    prefetch_factor: 4
    # Keep worker processes alive after consuming the dataset once
    persistent_workers: true
    # Drop the last batch if it is incomplete, i.e., smaller than the batch size
    drop_last: true
# Evaluation hyperparameters
eval:
  # Evaluation batch size - does not influence the result, just speeds up the
  # evaluation by doing more in parallel
  batch_size: 4096
  # DataLoader keyword arguments
  loader:
    # Number of workers to use for loading the data in parallel
    num_workers: 10
    # Number of batches loaded in advance by each worker
    prefetch_factor: 4
    # Do not drop the last batch if it is incomplete
    drop_last: false
# Model to ONNX export hyperparameters
export:
  # Version of the default ONNX opset
  opset_version: 14
  # Apply the constant-folding optimization
  do_constant_folding: true
# FPGA dataflow accelerator build configuration
build:
  # FINN compiler configurations
  finn:
    # Directory to store the build outputs
    output_dir: outputs/build
    # Run synthesis to generate a .dcp for the stitched-IP output product
    stitched_ip_gen_dcp: false
    # Target clock period, i.e., inverse of target frequency
    synth_clk_period_ns: 10.0
    # Board to target with the build
    board: RFSoC2x2
    # Target shell flow: 'vivado_zynq' or 'vitis_alveo'
    shell_flow_type: vivado_zynq
    # Path to folding configuration file
    folding_config_file: folding.yaml
    # Path to layer implementation style specialization config
    specialize_layers_config_file: specialize_layers.json
    # Force the implementation of standalone thresholds to be able to use RTL
    # implementation of the MVU
    standalone_thresholds: true
    # Maximum bit-width of quantizers converted to multi-thresholds
    max_multithreshold_bit_width: 16
    # Maximum width of MVAU stream per PE
    mvau_wwidth_max: 2048
    # FIFO nodes with depth larger than 32768 will be split
    split_large_fifos: true
    #  # Optional: Start the build from a specific step
    #  start_step: "step_tidy_up_pre_attention"
    #  # Optional: Stop the build after a specific step
    #  stop_step: "step_hw_ipgen"
  # Build metrics aggregation configuration
  metrics:
    # Path to the report file to be summarized
    # Note: remember to adjust the build directory when changing the config
    # above
    report: outputs/build/report/post_synth_resources.json
    # Filter the report rows
    filter: (top)
# Accelerator deployment parameters
eval-on-device:
  # Accelerator batch size - should not influence the accuracy result, just
  # sends more evaluation data at once into the accelerator and thus might
  # influence the measured throughput
  batch_size: 4096
  # DataLoader keyword arguments
  loader:
    # Number of workers to use for loading the data in parallel
    num_workers: 4
    # Number of batches loaded in advance by each worker
    prefetch_factor: 4
    # Do not drop the last batch if it is incomplete
    drop_last: false
