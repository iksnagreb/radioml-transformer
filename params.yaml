# Global seed to make training and evaluation reproducible
# Note: Dataset splitting has its own seed
seed: 12
# Model configuration section
model:
  # Number of attention heads
  num_heads: 8
  # Number of attention block "layers"
  num_layers: 2
  # Enables/disables bias on linear layers
  bias: True
  # Size of the input/output embedding dimension
  emb_dim: 64
  # Size of the MLP layer dimension
  mlp_dim: 256
  # Type of normalization layer to use in the transformer blocks
  #   Options are: layer-norm, batch-norm and none
  norm: "batch-norm"
  # Dropout: probability of an element to be zeroed during training
  dropout: 0.0
  # Number of bits to use for quantized representation in "almost all layers"
  bits: 2
  # Quantization bit-width at the model inputs: Typically this should
  # be higher than for most other layers, e.g., keep this at 8 bits
  input_bits: 8
  # Quantization bit-width at the model outputs: Typically this should
  # be higher than for most other layers, e.g., keep this at 8 bits
  output_bits: 4
  # Number of classes at the classification head
  num_classes: 24
# Training/Validation dataset configuration
dataset:
  # Path to the dataset file
  path: "data/GOLD_XYZ_OSC.0001_1024.hdf5"
  # Optionally select only a subset of the classes
  #  classes: null
  # Optionally select only a subset of the available noise levels
  signal_to_noise_ratios: [
    # Noise levels from -6 dB upwards to 30 dB in steps of 2
    #   !!python/object/apply:builtins.range [-6, 31, 2]
    -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30
  ]
  # Splits the dataset into train/validation/evaluation subsets
  splits: [ 0.80, 0.10, 0.10 ]  # 80%, 20%, 20% splits
  # Seed used for reproducible splitting of datasets
  seed: 12
  # Reshape the dataset to fit the expected embedding dimension of the model
  # and to effectively reduce the sequence length
  reshape: [ -1, 64 ]  # -1 will infer the sequence length
# Training hyperparameters
train:
  # Batch size for training
  batch_size: 512
  # Number of training epochs to run
  epochs: 100
  # Optimizer configuration
  optimizer:
    # Name of the optimization algorithm to use
    algorithm: "adam"
    # (Initial) Learning rate
    lr: 0.001
    # L2 regularization
    weight_decay: 1e-05
    # Coefficients used for computing running averages of gradient and its
    # square.
    # Note: Set according to Vaswani et al. 2017
    betas: [ 0.9, 0.98 ]
    # Term added to the denominator to improve numerical stability
    # Note: Set according to Vaswani et al. 2017
    eps: 1.0e-9
  # Loss function to use
  criterion: "cross-entropy"
  # DataLoader keyword arguments
  loader:
    # Reshuffle data every epoch
    shuffle: True
    # Number of workers to use for loading the data in parallel
    num_workers: 10
    # Number of batches loaded in advance by each worker
    prefetch_factor: 4
    # Keep worker processes alive after consuming the dataset once
    persistent_workers: True
    # Drop the last batch if it is incomplete, i.e., smaller than the batch size
    drop_last: True
# Evaluation hyperparameters
eval:
  # Evaluation batch size - does not influence the result, just speeds up the
  # evaluation by doing more in parallel
  batch_size: 4096
  # DataLoader keyword arguments
  loader:
    # Number of workers to use for loading the data in parallel
    num_workers: 10
    # Number of batches loaded in advance by each worker
    prefetch_factor: 4
    # Do not drop the last batch if it is incomplete
    drop_last: False
# Model to ONNX export hyperparameters
export:
  # Version of the default ONNX opset
  opset_version: 14
  # Apply the constant-folding optimization
  do_constant_folding: True
